{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPir_6bCCFnZ"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wingated/cs473/blob/main/labs/cs473_lab_week_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a><p><b>After clicking the \"Open in Colab\" link, copy the notebook to your own Google Drive before getting started, or it will not save your work</b></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_slaQdUGCB0t"
      },
      "source": [
        "# BYU CS 473 Lab Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct7fnkcnCL8O"
      },
      "source": [
        "## Introduction:\n",
        "Welcome to your first lab for CS 473, Advanced Machine Learning.\n",
        "\n",
        "In machine learning, models often predict *unnormalized log probabilities*. These must often be converted into regular probabilities.\n",
        "\n",
        "In this lab, you will explore the log-sum-exp function, which is described in the text (Sec. 2.5.4).  You will code up several variants of the function, and compare their performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUat5xRAcdrC"
      },
      "source": [
        "# Part 1: Logsumexp\n",
        "---\n",
        "## Setup: The Iris Dataset\n",
        "We'll begin by downloading the Iris dataset. The iris dataset is a simple, but very famous, dataset introduced to the world by RA Fisher (the “father” of modern statistics”) in 1939. The dataset has five columns:\n",
        "* sepal length (cm)\n",
        "* sepal width (cm)\n",
        "* petal length (cm)\n",
        "* petal width (cm)\n",
        "* class\n",
        "\n",
        "In order to get logits to play with, we'll first train a multinomial logistic regression model (Sec. 2.5.3).  This model naturally outputs logits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "j1m2KIHShNdC"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "ds = datasets.load_dataset( \"scikit-learn/iris\" )\n",
        "\n",
        "df = pd.DataFrame( ds['train'] )\n",
        "\n",
        "X = np.array( df[['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']] )\n",
        "Y = np.array( LabelEncoder().fit_transform( df['Species'] ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "l5hV6PS0CwT8"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression().fit(X,Y)\n",
        "\n",
        "W = model.coef_\n",
        "b = model.intercept_\n",
        "\n",
        "b = np.reshape( b, (3,1))\n",
        "\n",
        "logits = np.dot( W, X.T ) + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Esta7gBS1v"
      },
      "source": [
        "---\n",
        "## Exercise 1: convert logits to probabilities\n",
        "\n",
        "Since our model outputs logits, they must be converted. To do this, we'll use the softmax function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "E3sgOg5oAvCv"
      },
      "outputs": [],
      "source": [
        "def softmax( logits ):\n",
        "    # logits is a numpy matrix of d x N\n",
        "    # where\n",
        "    #   d is the number of classes\n",
        "    #   N is the number of data points\n",
        "    # use equation 2.99 (see also Eq. 2.94)\n",
        "\n",
        "    exp_logits = np.exp(logits)\n",
        "    Z_a = np.sum(exp_logits, axis=0, keepdims=True)\n",
        "    probabilities = exp_logits / Z_a\n",
        "\n",
        "    return probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzxcqJE-XjLJ",
        "outputId": "d6028b8f-c04b-44d5-ba66-70c43ba27b83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probs[:,0] = [9.81747643e-01 1.82523424e-02 1.43429404e-08]\n",
            "Expected:   [9.81803910e-01, 1.81960759e-02, 1.43430317e-08]\n",
            "\n",
            "probs[:,120] = [5.54234723e-06 2.39030160e-02 9.76091442e-01]\n",
            "Expected:     [5.49519371e-06, 2.38812718e-02, 9.76113233e-01]\n",
            "\n",
            "Sum of probs[:,0] = 0.9999999999999999\n",
            "Sum of probs[:,120] = 1.0\n",
            "(Should both be 1.0)\n"
          ]
        }
      ],
      "source": [
        "# Test case 1: probs[:,0]\n",
        "probs = softmax(logits)\n",
        "print(\"probs[:,0] =\", probs[:,0])\n",
        "print(\"Expected:   [9.81803910e-01, 1.81960759e-02, 1.43430317e-08]\")\n",
        "print()\n",
        "\n",
        "# Test case 2: probs[:,120]\n",
        "print(\"probs[:,120] =\", probs[:,120])\n",
        "print(\"Expected:     [5.49519371e-06, 2.38812718e-02, 9.76113233e-01]\")\n",
        "print()\n",
        "\n",
        "# Verify probabilities sum to 1\n",
        "print(\"Sum of probs[:,0] =\", np.sum(probs[:,0]))\n",
        "print(\"Sum of probs[:,120] =\", np.sum(probs[:,120]))\n",
        "print(\"(Should both be 1.0)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoXi9mq3BS1w"
      },
      "source": [
        "### test cases\n",
        "probs = softmax( logits )\n",
        "probs[:,0]\n",
        "#### array([9.81803910e-01, 1.81960759e-02, 1.43430317e-08])\n",
        "probs[:,120]\n",
        "#### array([5.49519371e-06, 2.38812718e-02, 9.76113233e-01])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Q64wxOyBS1w"
      },
      "source": [
        "---\n",
        "## Exercise 2: convert logits to probabilities\n",
        "\n",
        "Now, code up the logsumexp function.  What test cases should you use for this function?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zFzz8bqZBS1w"
      },
      "outputs": [],
      "source": [
        "def logsumexp( logits ):\n",
        "    # logits is a numpy matrix of d x N\n",
        "    # where\n",
        "    #   d is the number of classes\n",
        "    #   N is the number of data points\n",
        "    # use equation 2.100\n",
        "\n",
        "    m = np.max(logits, axis=0, keepdims=True)\n",
        "    shifted_logits = logits - m\n",
        "    log_sum_exp_shifted = np.log(np.sum(np.exp(shifted_logits), axis=0, keepdims=True))\n",
        "\n",
        "    return m + log_sum_exp_shifted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "7sGtB6lNBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd50b23-b875-43d1-c091-e996e9067ba3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "probs[:,0] =  [7.36063027]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# test cases\n",
        "probs = logsumexp( logits )\n",
        "probs[:,0]\n",
        "print(\"probs[:,0] = \", probs[:,0])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVJ0YvddXjLJ"
      },
      "source": [
        "What should be printed??\n",
        "\n",
        "[7.36063027]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aR9B45o8BS1w"
      },
      "source": [
        "---\n",
        "## Exercise 3: explore underflow / overlow\n",
        "\n",
        "First, code up a function that compares two distributions. This can be anything you want; you may consider things like the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "REO87dlmBS1w"
      },
      "outputs": [],
      "source": [
        "def compare_probs( probs1, probs2 ):\n",
        "  if probs2.shape[0] == 1:  # logsumexp returns (1, N)\n",
        "      probs2_as_probs = np.exp(logits - probs2)\n",
        "      return np.mean((probs1 - probs2_as_probs) ** 2)\n",
        "  else:\n",
        "      return np.mean((probs1 - probs2) ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "3A85oOGUBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ed355c9-1f1c-415d-e6a6-aad511081ee9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.834799520780957e-32)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "probs1 = softmax( logits )\n",
        "probs2 = logsumexp( logits )\n",
        "compare_probs( probs1, probs2 )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "niNMrfHJBS1w"
      },
      "source": [
        "Now, see what happens if you add (or subtract) a constant from logits. How big must the constant be before things start going haywire?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "rYmP1Jj7BS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7c8866-4360-400c-cd4e-7857408f455d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing overflow/underflow with different constants: \n",
            "\n",
            "Testing C = 0:\n",
            "  Softmax: overflow/nan = False\n",
            "  Logsumexp: overflow/nan = False\n",
            "    Sample values: [7.36063027]\n",
            "\n",
            "Testing C = 100:\n",
            "  Softmax: overflow/nan = False\n",
            "  Logsumexp: overflow/nan = False\n",
            "    Sample values: [107.36063027]\n",
            "\n",
            "Testing C = 500:\n",
            "  Softmax: overflow/nan = False\n",
            "  Logsumexp: overflow/nan = False\n",
            "    Sample values: [507.36063027]\n",
            "\n",
            "Testing C = 700:\n",
            "  Softmax: overflow/nan = False\n",
            "  Logsumexp: overflow/nan = False\n",
            "    Sample values: [707.36063027]\n",
            "\n",
            "Testing C = 710:\n",
            "  Softmax: overflow/nan = True\n",
            "    Sample values: [nan nan  0.]\n",
            "  Logsumexp: overflow/nan = False\n",
            "    Sample values: [717.36063027]\n",
            "\n",
            "Testing C = 720:\n",
            "  Softmax: overflow/nan = True\n",
            "    Sample values: [nan nan  0.]\n",
            "  Logsumexp: overflow/nan = False\n",
            "    Sample values: [727.36063027]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2289847087.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  exp_logits = np.exp(logits)\n",
            "/tmp/ipython-input-2289847087.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  probabilities = exp_logits / Z_a\n"
          ]
        }
      ],
      "source": [
        "print(\"Testing overflow/underflow with different constants: \")\n",
        "constants = [0, 100, 500, 700, 710, 720]\n",
        "\n",
        "for C in constants:\n",
        "  print(f\"\\nTesting C = {C}:\")\n",
        "\n",
        "  try:\n",
        "      probs1 = softmax(logits + C)\n",
        "      has_overflow_softmax = np.any(np.isinf(probs1)) or np.any(np.isnan(probs1))\n",
        "      print(f\"  Softmax: overflow/nan = {has_overflow_softmax}\")\n",
        "      if has_overflow_softmax:\n",
        "          print(f\"    Sample values: {probs1[:,0]}\")\n",
        "  except Exception as e:\n",
        "      print(f\"  Softmax: Failed with exception: {e}\")\n",
        "\n",
        "  try:\n",
        "      probs2 = logsumexp(logits + C)\n",
        "      has_overflow_logsumexp = np.any(np.isinf(probs2)) or np.any(np.isnan(probs2))\n",
        "      print(f\"  Logsumexp: overflow/nan = {has_overflow_logsumexp}\")\n",
        "      if not has_overflow_logsumexp:\n",
        "          print(f\"    Sample values: {probs2[:,0]}\")\n",
        "  except Exception as e:\n",
        "      print(f\"  Logsumexp: Failed with exception: {e}\")\n",
        "# etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaPkTeNBS1w"
      },
      "source": [
        "Now convert the logits to 16-bit precision, and re-run your experiments. Analyze the differences you see (2-3 sentences)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6oxJEheOBS1w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fbc01fa-c46b-4b96-8dba-6c4ce43ea0f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing C = 0 with float16:\n",
            "  Softmax: overflow/nan = False\n",
            "  Logsumexp: overflow/nan = False\n",
            "\n",
            "Testing C = 10 with float16:\n",
            "  Softmax: overflow/nan = True\n",
            "    First few values: [nan nan  0.]\n",
            "  Logsumexp: overflow/nan = False\n",
            "\n",
            "Testing C = 15 with float16:\n",
            "  Softmax: overflow/nan = True\n",
            "    First few values: [nan nan  0.]\n",
            "  Logsumexp: overflow/nan = False\n",
            "\n",
            "Testing C = 20 with float16:\n",
            "  Softmax: overflow/nan = True\n",
            "    First few values: [nan nan  0.]\n",
            "  Logsumexp: overflow/nan = False\n",
            "\n",
            "Testing C = 25 with float16:\n",
            "  Softmax: overflow/nan = True\n",
            "    First few values: [nan nan nan]\n",
            "  Logsumexp: overflow/nan = False\n",
            "\n",
            "Testing C = 30 with float16:\n",
            "  Softmax: overflow/nan = True\n",
            "    First few values: [nan nan nan]\n",
            "  Logsumexp: overflow/nan = False\n",
            "\n",
            "Analysis:\n",
            "(Your analysis here)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2289847087.py:8: RuntimeWarning: overflow encountered in exp\n",
            "  exp_logits = np.exp(logits)\n",
            "/tmp/ipython-input-2289847087.py:10: RuntimeWarning: invalid value encountered in divide\n",
            "  probabilities = exp_logits / Z_a\n"
          ]
        }
      ],
      "source": [
        "logits = logits.astype( np.float16 )\n",
        "\n",
        "constants_16 = [0, 10, 15, 20, 25, 30]\n",
        "\n",
        "\n",
        "for C in constants_16:\n",
        "    print(f\"\\nTesting C = {C} with float16:\")\n",
        "\n",
        "    try:\n",
        "        probs1 = softmax(logits + C)\n",
        "        has_overflow_1 = np.any(np.isinf(probs1)) or np.any(np.isnan(probs1))\n",
        "        print(f\"  Softmax: overflow/nan = {has_overflow_1}\")\n",
        "        if has_overflow_1:\n",
        "            print(f\"    First few values: {probs1[:,0]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Softmax: Exception - {e}\")\n",
        "\n",
        "    try:\n",
        "        probs2 = logsumexp(logits + C)\n",
        "        has_overflow_2 = np.any(np.isinf(probs2)) or np.any(np.isnan(probs2))\n",
        "        print(f\"  Logsumexp: overflow/nan = {has_overflow_2}\")\n",
        "        if has_overflow_2:\n",
        "            print(f\"    First few values: {probs2[:,0]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Logsumexp: Exception - {e}\")\n",
        "\n",
        "# Analysis\n",
        "print(\"\\nAnalysis:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJZaJKFvXjLJ"
      },
      "source": [
        "### Analysis\n",
        "\n",
        "With float64 precision, softmax remained stable up to C=700 and failed at C=710. With float16 precision, softmax failed much earlier at C=10, showing how reduced precision dramatically decreases numerical stability. However, logsumexp remained stable in both cases, demonstrating the effectiveness of the numerical stability technique in equation 2.100. The dramatic difference between failure points shows that float16's limited range makes overflow much more likely even with modest increases to the input values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UZs7xOKBS1w"
      },
      "source": [
        "---\n",
        "## Exercise 4: cleanly compute log probabilities\n",
        "\n",
        "Sometimes, we want to compute log probabilities (which are different from logits), but we want to do so \"cleanly\", ie, while avoiding overflow / underflow. First, mathematically figure out what the log of the softmax is (ie, take the log of eq. 2.99), and then combine it with insights from coding up the logsumexp function. Hint: at the end of the day, you will simply shift each column by a per-column constant!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Laus2v79BS1x"
      },
      "outputs": [],
      "source": [
        "def log_logsumexp( logits ):\n",
        "    # logits is a numpy matrix of d x N\n",
        "    # where\n",
        "    #   d is the number of classes\n",
        "    #   N is the number of data points\n",
        "\n",
        "    # your code here\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62UvuyhYBS1x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxX04mG2zXOe"
      },
      "source": [
        "---\n",
        "# Part 2: Probability Fundamentals\n",
        "\n",
        "For the following exercises, you are encouraged to work both by hand and by code however makes the most sense.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xsg221sSR_4"
      },
      "source": [
        "## Exercise 1a: Joint Probability Distributions\n",
        "\n",
        "You are given the following two binary variables, X and Y, that can each take on the values 0 or 1. Assuming X and Y are independent, calculate the joint probability table (2x2 table for P(X, Y)). Display as a numpy array.\n",
        "\n",
        "P(X=0) = 0.6\n",
        "\n",
        "P(X=1) = 0.4\n",
        "\n",
        "P(Y=0) = 0.7\n",
        "\n",
        "P(Y=1) = 0.3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V7dgYZfrSQ22"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKat0t8w3qWY"
      },
      "source": [
        "Next, compute the following conditional probabilities:\n",
        "\n",
        "P(X=0|Y=0) = ?\n",
        "\n",
        "P(X=0|Y=1) = ?\n",
        "\n",
        "P(Y=0|X=0) = ?\n",
        "\n",
        "P(Y=0|X=1) = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoFygm9S3yPQ"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO7ikQFrXjLK"
      },
      "source": [
        "Compare the result of these conditional probabilities to the original marginal probabilities given. What does this say about the relationship between variable dependence and using conditional probabilities? Write 1-2 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neW3qyGTXjLK"
      },
      "source": [
        "(Your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERItpoAmXjLK"
      },
      "source": [
        "## Exercise 1b: Joint Probability Distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKFH77LVXjLK"
      },
      "source": [
        "Now consider this joint distribution:\n",
        "\n",
        "|  | $Y = 0$ | $Y = 1$|\n",
        "| :------- | :------: | -------: |\n",
        "| $X = 0$  | 0.45  | 0.10  |\n",
        "| $X = 1$  | 0.25  | 0.20  |\n",
        "\n",
        "First, compute the marginals from the joint table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhRNmbKwXjLK"
      },
      "outputs": [],
      "source": [
        "joint_prob_table = np.array([[0.45, 0.10],\n",
        "                             [0.25, 0.20]])\n",
        "\n",
        "# P(X=0) = ?\n",
        "# P(X=1) = ?\n",
        "# P(Y=0) = ?\n",
        "# P(Y=1) = ?\n",
        "\n",
        "# Your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWPAq6W1XjLK"
      },
      "source": [
        "Compute the same conditional probabilities as above:\n",
        "\n",
        "P(X=0|Y=0) = ?\n",
        "\n",
        "P(X=0|Y=1) = ?\n",
        "\n",
        "P(Y=0|X=0) = ?\n",
        "\n",
        "P(Y=0|X=1) = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcg9XZhqXjLK"
      },
      "outputs": [],
      "source": [
        "# P(X=0|Y=0) = ?\n",
        "# P(X=0|Y=1) = ?\n",
        "# P(Y=0|X=0) = ?\n",
        "# P(Y=0|X=1) = ?\n",
        "\n",
        "# Your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jw2DFXVXjLK"
      },
      "source": [
        "Check if the independence property $P(X, Y) = P(X)P(Y)$ holds for any cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQfMEQ5YXjLK"
      },
      "outputs": [],
      "source": [
        "# Your answer here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGNAJVxqXjLM"
      },
      "source": [
        "Compare P(X=0|Y=0) to P(X=0|Y=1), and discuss what this says about the dependence between these variables (1-2 sentences)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QxBrkv1XjLM"
      },
      "source": [
        "(Your answer here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0VjBd8qXjLM"
      },
      "source": [
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elOEoNBWTOUj"
      },
      "source": [
        "---\n",
        "## Exercise 2: Bayes Theorem\n",
        "\n",
        "After your yearly checkup, the doctor has bad news and good news. The bad news is that you tested positive\n",
        "for a serious disease, and that the test is 99% accurate (i.e., the probability of testing positive given that you\n",
        "have the disease is 0.99, as is the probability of testing negative given that you don’t have the disease). The\n",
        "good news is that this is a rare disease, striking only one in 10,000 people. What are the chances that you\n",
        "actually have the disease? (Show your calculations as well as giving the final result.)\n",
        "\n",
        "*Hint: write out the variables you know, and think about what you'll need to calculate to find the final answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWWmJ6FbCyyH"
      },
      "outputs": [],
      "source": [
        "# Your answer/work here\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}